{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29434006",
   "metadata": {},
   "source": [
    "# Forecasting the Effective Federal Funds Rate\n",
    "In this notebook, I explore how to use macroeconomic indicators to predict the Effective Federal Funds Rate (EFFR) using various machine learning models.\n",
    "\n",
    "The workflow includes:\n",
    "- Loading and preparing the data\n",
    "- Feature engineering\n",
    "- Training models (Random Forest, Gradient Boosting, etc.)\n",
    "- Evaluating performance\n",
    "- Visualizing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "# loading the dataset\n",
    "index_data = pd.read_csv(\"index 2.csv\")\n",
    "# loading the dataset\n",
    "cpi_data = pd.read_csv(\"US CPI.csv\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "index_data['date'] = pd.to_datetime(index_data[['Year', 'Month', 'Day']])\n",
    "cpi_data['date'] = pd.to_datetime(cpi_data['Yearmon'], format='%d-%m-%Y')\n",
    "\n",
    "# Merge CPI into index data\n",
    "# combining datasets\n",
    "merged_df = pd.merge(index_data, cpi_data[['date', 'CPI']], on='date', how='left')\n",
    "\n",
    "# Interpolate macroeconomic features (forward fill where safe)\n",
    "# filling in missing values\n",
    "merged_df['Inflation Rate'] = merged_df['Inflation Rate'].interpolate(method='linear', limit_direction='forward')\n",
    "# filling in missing values\n",
    "merged_df['Unemployment Rate'] = merged_df['Unemployment Rate'].interpolate(method='linear', limit_direction='forward')\n",
    "# filling in missing values\n",
    "merged_df['CPI'] = merged_df['CPI'].interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "# Drop rows where the target variable is missing\n",
    "# combining datasets\n",
    "merged_df = merged_df[merged_df['Effective Federal Funds Rate'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Forward-fill Real GDP (quarterly → monthly)\n",
    "# combining datasets\n",
    "if 'Real GDP (Percent Change)' in merged_df.columns:\n",
    "# combining datasets\n",
    "    merged_df['Real GDP (Percent Change)'] = merged_df['Real GDP (Percent Change)'].fillna(method='ffill')\n",
    "\n",
    "    columns_to_drop = [\n",
    "    'Federal Funds Target Rate',\n",
    "    'Federal Funds Upper Target',\n",
    "    'Federal Funds Lower Target',\n",
    "    'Year', 'Month', 'Day' ]\n",
    "    \n",
    "# combining datasets\n",
    "merged_df = merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns])\n",
    "\n",
    "# Final cleaned dataset\n",
    "# combining datasets\n",
    "cleaned_df = merged_df\n",
    "\n",
    "cleaned_df.to_csv(\"cleaned_effr_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b05d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature expansion for EFFR modeling (robust, no-leakage) ===\n",
    "# - Builds on your existing script\n",
    "# - Creates lags, rolling stats, change/momentum, seasonality, and safe interactions\n",
    "# - Only uses columns that actually exist in your file\n",
    "#\n",
    "# Output: features_effr_data_extended.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# ---------- Load ----------\n",
    "# loading the dataset\n",
    "df = pd.read_csv(\"cleaned_effr_data.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---------- Helper ----------\n",
    "def cols_present(candidates):\n",
    "    return [c for c in candidates if c in df.columns]\n",
    "\n",
    "# Core columns you already have\n",
    "base_cols = cols_present([\"CPI\", \"Inflation Rate\", \"Unemployment Rate\", \"Real GDP (Percent Change)\"])\n",
    "\n",
    "# If your dataset also contains any of these, they’ll be used automatically:\n",
    "optional_cols = cols_present([\n",
    "    \"SOFR\", \"2Y Yield\", \"10Y Yield\", \"Federal Funds Target Rate\",\n",
    "    \"Federal Funds Upper Target\", \"Federal Funds Lower Target\",\n",
    "    \"Industrial Production\", \"ISM Manufacturing PMI\", \"ISM Services PMI\",\n",
    "    \"Retail Sales\", \"PCE\", \"Core PCE\", \"PPI\"\n",
    "])\n",
    "\n",
    "use_cols = base_cols + optional_cols\n",
    "if not use_cols:\n",
    "    raise ValueError(\"No known numeric macro columns found. Check column names in cleaned_effr_data.csv\")\n",
    "\n",
    "# Align numeric-only features (avoid object columns sneaking in)\n",
    "for c in use_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# To be conservative about data availability (and reporting lags), we shift most derived features by 1 period\n",
    "SHIFT = 1\n",
    "\n",
    "# ---------- 1) Lags ----------\n",
    "lags = [1, 3, 6, 12]\n",
    "for col in use_cols:\n",
    "    for L in lags:\n",
    "        df[f\"{col}_lag{L}\"] = df[col].shift(L)\n",
    "\n",
    "# ---------- 2) Rolling means / volatility (past-only) ----------\n",
    "windows = [3, 6, 12]\n",
    "for col in use_cols:\n",
    "    s = df[col]\n",
    "    for w in windows:\n",
    "        df[f\"{col}_ma{w}\"]  = s.rolling(w, min_periods=w).mean().shift(SHIFT)\n",
    "        df[f\"{col}_std{w}\"] = s.rolling(w, min_periods=w).std().shift(SHIFT)\n",
    "\n",
    "# ---------- 3) Change-based features (momentum) ----------\n",
    "for col in use_cols:\n",
    "    s = df[col]\n",
    "    # Level changes\n",
    "    df[f\"{col}_diff1\"]  = s.diff(1).shift(SHIFT)     # MoM change, shifted\n",
    "    df[f\"{col}_diff12\"] = s.diff(12).shift(SHIFT)    # YoY change, shifted\n",
    "    # Percent changes (safe with small eps)\n",
    "    eps = 1e-9\n",
    "    df[f\"{col}_pct1\"]  = (s.pct_change(1)).shift(SHIFT)\n",
    "    df[f\"{col}_pct12\"] = (s.pct_change(12)).shift(SHIFT)\n",
    "    # Momentum: short MA minus long MA\n",
    "    df[f\"{col}_mom_ma3_12\"] = (s.rolling(3, min_periods=3).mean()\n",
    "                                - s.rolling(12, min_periods=12).mean()).shift(SHIFT)\n",
    "\n",
    "# ---------- 4) Rolling z-scores (level vs local mean) ----------\n",
    "for col in use_cols:\n",
    "    roll_mean = df[col].rolling(12, min_periods=12).mean()\n",
    "    roll_std  = df[col].rolling(12, min_periods=12).std()\n",
    "    df[f\"{col}_z12\"] = ((df[col] - roll_mean) / (roll_std.replace(0, np.nan))).shift(SHIFT)\n",
    "\n",
    "# ---------- 5) Safe interactions (lagged to avoid leakage) ----------\n",
    "# Interact a small subset to control dimensionality:\n",
    "interaction_pool = cols_present([\"CPI\", \"Inflation Rate\", \"Unemployment Rate\"]) or use_cols[:3]\n",
    "for a, b in combinations(interaction_pool, 2):\n",
    "    df[f\"{a}_x_{b}\"] = (df[a].shift(SHIFT) * df[b].shift(SHIFT))\n",
    "\n",
    "# ---------- 6) Seasonality & calendar dummies ----------\n",
    "df[\"month\"]   = df[\"date\"].dt.month\n",
    "df[\"quarter\"] = df[\"date\"].dt.quarter\n",
    "# Cyclical encoding\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "# Quarter-end liquidity pressures\n",
    "df[\"is_qtr_end\"] = (df[\"date\"].dt.month.isin([3, 6, 9, 12])).astype(int)\n",
    "# Year-end dummy\n",
    "df[\"is_december\"] = (df[\"date\"].dt.month == 12).astype(int)\n",
    "\n",
    "# ---------- 7) Yield curve (if yields available) ----------\n",
    "if {\"10Y Yield\", \"2Y Yield\"}.issubset(df.columns):\n",
    "    df[\"yc_10y_2y\"] = (pd.to_numeric(df[\"10Y Yield\"], errors=\"coerce\")\n",
    "                       - pd.to_numeric(df[\"2Y Yield\"], errors=\"coerce\")).shift(SHIFT)\n",
    "\n",
    "# ---------- Finalize ----------\n",
    "# Keep rows where at least one of our engineered features exists and target is present\n",
    "target_col = \"Effective Federal Funds Rate\"\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(\"Target column 'Effective Federal Funds Rate' not found in cleaned_effr_data.csv\")\n",
    "\n",
    "# Build a list of engineered columns to enforce non-NA rows\n",
    "engineered_prefixes = [\n",
    "    \"_lag\", \"_ma\", \"_std\", \"_diff\", \"_pct\", \"_mom_ma3_12\", \"_z12\", \"_x_\", \"month_\", \"yc_10y_2y\"\n",
    "]\n",
    "engineered_cols = [c for c in df.columns if any(p in c for p in engineered_prefixes)]\n",
    "# Also include simple calendar flags\n",
    "engineered_cols += [\"month\", \"quarter\", \"is_qtr_end\", \"is_december\"]\n",
    "\n",
    "# Drop rows with NA from engineered features or target\n",
    "df_out = df.dropna(subset=engineered_cols + [target_col]).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "out_path = \"features_effr_data_extended.csv\"\n",
    "df_out.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved {out_path} with {len(df_out)} rows and {df_out.shape[1]} columns.\")\n",
    "print(f\"Engineered {len(engineered_cols)} feature columns (plus calendar features).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "IN_PATH  = \"features_effr_data_extended.csv\"\n",
    "TARGET   = \"Effective Federal Funds Rate\"\n",
    "TOP_N    = 25  # change as needed\n",
    "OUT_IMPORTANCES = \"feature_importances_full.csv\"\n",
    "OUT_TOP_RAW     = f\"features_effr_top{TOP_N}.csv\"\n",
    "OUT_TOP_CLEAN   = f\"features_effr_top{TOP_N}_clean.csv\"\n",
    "\n",
    "# ---------------- Load ----------------\n",
    "# loading the dataset\n",
    "df = pd.read_csv(IN_PATH)\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in {IN_PATH}\")\n",
    "\n",
    "# Keep only numeric features for ranking\n",
    "exclude = [\"date\", TARGET]\n",
    "X_raw = df.drop(columns=[c for c in exclude if c in df.columns], errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# Align and drop rows with missing target\n",
    "mask = y.notna()\n",
    "X_raw, y = X_raw.loc[mask], y.loc[mask]\n",
    "\n",
    "# ---------------- Clean numeric X for modeling ----------------\n",
    "# 1) Replace +/-inf with NaN\n",
    "X = X_raw.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 2) Optional: clip extreme outliers per column (0.1%–99.9%) before imputation\n",
    "q_low  = X.quantile(0.001)\n",
    "q_high = X.quantile(0.999)\n",
    "X = X.clip(lower=q_low, upper=q_high, axis=1)\n",
    "\n",
    "# 3) Median impute remaining NaNs\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index).astype(np.float64)\n",
    "\n",
    "# 4) Drop zero-variance columns (if any)\n",
    "var = X_imp.var(axis=0)\n",
    "keep_cols = var[var > 0].index.tolist()\n",
    "X_imp = X_imp[keep_cols]\n",
    "\n",
    "# Safety check: ensure finiteness\n",
    "if not np.isfinite(X_imp.to_numpy()).all():\n",
    "    raise ValueError(\"Non-finite values remain after cleaning. Inspect the source data.\")\n",
    "\n",
    "# ---------------- Rank features with RF ----------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_imp, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_imp.columns).sort_values(ascending=False)\n",
    "importances.to_csv(OUT_IMPORTANCES, header=[\"importance\"])\n",
    "print(f\"Saved feature importances to {OUT_IMPORTANCES}\")\n",
    "\n",
    "# ---------------- Select top N & save datasets ----------------\n",
    "top_features = importances.head(TOP_N).index.tolist()\n",
    "print(f\"Top {TOP_N} features:\\n{top_features}\")\n",
    "\n",
    "# RAW version (original values)\n",
    "cols_raw = [c for c in [\"date\", TARGET] if c in df.columns] + top_features\n",
    "df_top_raw = df.loc[mask, cols_raw]  # same rows as used for ranking\n",
    "df_top_raw.to_csv(OUT_TOP_RAW, index=False)\n",
    "print(f\"Saved raw top-{TOP_N} dataset to {OUT_TOP_RAW}\")\n",
    "\n",
    "# CLEAN version (imputed, clipped, finite) — ready for modeling\n",
    "# Rebuild a clean frame with date/target plus cleaned top features\n",
    "df_top_clean = pd.DataFrame(index=X_imp.index)\n",
    "if \"date\" in df.columns:\n",
    "    df_top_clean[\"date\"] = pd.to_datetime(df.loc[mask, \"date\"]).values\n",
    "df_top_clean[TARGET] = y.values\n",
    "df_top_clean[top_features] = X_imp[top_features].values\n",
    "df_top_clean.to_csv(OUT_TOP_CLEAN, index=False)\n",
    "print(f\"Saved CLEAN top-{TOP_N} dataset to {OUT_TOP_CLEAN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & compare RF / LightGBM / CatBoost, with XGBoost as a best‑effort optional add.\n",
    "# Designed to work even with very old xgboost builds (no eval_metric / no early_stopping in fit).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ---------------- Optional libs (skip silently if missing) ----------------\n",
    "HAS_XGB = True\n",
    "HAS_LGBM = True\n",
    "HAS_CAT  = True\n",
    "xgb_ver = None\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_ver = getattr(xgb, \"__version__\", \"unknown\")\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "except Exception:\n",
    "    HAS_CAT = False\n",
    "\n",
    "# ---------------- Load & prep ----------------\n",
    "IN_PATH = \"features_effr_top25_clean.csv\"   # use the CLEAN file you created earlier\n",
    "TARGET  = \"Effective Federal Funds Rate\"\n",
    "\n",
    "# loading the dataset\n",
    "df = pd.read_csv(IN_PATH)\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target '{TARGET}' not found in {IN_PATH}\")\n",
    "\n",
    "# Keep only numeric features (drop date if present)\n",
    "exclude = [\"date\", TARGET]\n",
    "X = df.drop(columns=[c for c in exclude if c in df.columns], errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# Safety: drop any remaining NaN/Inf rows in target or features\n",
    "mask = y.notna()\n",
    "X, y = X.loc[mask], y.loc[mask]\n",
    "finite_mask = np.isfinite(X.values).all(axis=1)\n",
    "X, y = X.loc[finite_mask], y.loc[finite_mask]\n",
    "\n",
    "# Time-based split 80/20\n",
    "n = len(X)\n",
    "split_idx = int(n * 0.80)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Small validation tail from train (for early stopping where supported)\n",
    "val_size = max(1, int(len(X_train) * 0.10))\n",
    "X_tr, X_val = X_train.iloc[:-val_size], X_train.iloc[-val_size:]\n",
    "y_tr, y_val = y_train.iloc[:-val_size], y_train.iloc[-val_size:]\n",
    "\n",
    "def eval_preds(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return {\"RMSE\": float(np.sqrt(mse)),\n",
    "            \"MAE\":  float(mean_absolute_error(y_true, y_pred)),\n",
    "            \"R2\":   float(r2_score(y_true, y_pred))}\n",
    "\n",
    "metrics = []\n",
    "preds = pd.DataFrame(index=X_test.index)\n",
    "preds[\"Actual\"] = y_test.values\n",
    "\n",
    "# ---------------- Random Forest ----------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "# making predictions\n",
    "rf_pred = rf.predict(X_test)\n",
    "m = eval_preds(y_test, rf_pred); m[\"Model\"] = \"RandomForest\"\n",
    "metrics.append(m)\n",
    "preds[\"RF_Pred\"] = rf_pred\n",
    "pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\\\n",
    "  .to_csv(\"rf_importance.csv\", header=[\"importance\"])\n",
    "\n",
    "# ---------------- XGBoost (best-effort, fully version-safe) ----------------\n",
    "if HAS_XGB:\n",
    "    # Keep params minimal to maximize compatibility with older versions\n",
    "    xgbr = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=5,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Try with eval_set + early stopping; progressively fall back if unsupported\n",
    "    trained = False\n",
    "    try:\n",
    "        # Some old versions reject eval_metric / early_stopping_rounds / verbose in fit(),\n",
    "        # so DO NOT pass eval_metric; only pass eval_set & early_stopping_rounds first.\n",
    "        xgbr.fit(X_tr, y_tr,\n",
    "                 eval_set=[(X_val, y_val)],\n",
    "                 early_stopping_rounds=200)\n",
    "        trained = True\n",
    "    except TypeError:\n",
    "        try:\n",
    "            # No early stopping\n",
    "            xgbr.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n",
    "            trained = True\n",
    "        except TypeError:\n",
    "            # Plain fit on all train\n",
    "            xgbr.fit(X_train, y_train)\n",
    "            trained = True\n",
    "\n",
    "    if trained:\n",
    "# making predictions\n",
    "        xgb_pred = xgbr.predict(X_test)\n",
    "        m = eval_preds(y_test, xgb_pred); m[\"Model\"] = f\"XGBoost({xgb_ver})\"\n",
    "        metrics.append(m)\n",
    "        preds[\"XGB_Pred\"] = xgb_pred\n",
    "        # importance (fallback if attribute missing)\n",
    "        try:\n",
    "            imp = pd.Series(xgbr.feature_importances_, index=X.columns)\n",
    "        except Exception:\n",
    "            try:\n",
    "                booster = xgbr.get_booster()\n",
    "                score = booster.get_score(importance_type=\"weight\")\n",
    "                imp = pd.Series({col: score.get(f\"f{idx}\", 0.0) for idx, col in enumerate(X.columns)})\n",
    "            except Exception:\n",
    "                imp = pd.Series(dtype=float)\n",
    "        if not imp.empty:\n",
    "            imp.sort_values(ascending=False).to_csv(\"xgb_importance.csv\", header=[\"importance\"])\n",
    "\n",
    "# ---------------- LightGBM ----------------\n",
    "if HAS_LGBM:\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=96,\n",
    "        min_data_in_leaf=10,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Early stopping via callbacks (works on old & new LightGBM)\n",
    "    try:\n",
    "        lgbm.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(300, verbose=False)]\n",
    "        )\n",
    "    except TypeError:\n",
    "        lgbm.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[lgb.early_stopping(300)]\n",
    "        )\n",
    "# making predictions\n",
    "    lgb_pred = lgbm.predict(X_test)\n",
    "    m = eval_preds(y_test, lgb_pred); m[\"Model\"] = \"LightGBM\"\n",
    "    metrics.append(m)\n",
    "    preds[\"LGBM_Pred\"] = lgb_pred\n",
    "    pd.Series(lgbm.feature_importances_, index=X.columns).sort_values(ascending=False)\\\n",
    "      .to_csv(\"lgbm_importance.csv\", header=[\"importance\"])\n",
    "\n",
    "# ---------------- CatBoost ----------------\n",
    "if HAS_CAT:\n",
    "    cat = CatBoostRegressor(\n",
    "        iterations=4000,\n",
    "        learning_rate=0.01,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3.0,\n",
    "        loss_function=\"RMSE\",\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    # Older CatBoost accepts early_stopping_rounds; if not, fallback\n",
    "    try:\n",
    "        cat.fit(\n",
    "            Pool(X_tr, y_tr),\n",
    "            eval_set=Pool(X_val, y_val),\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=300\n",
    "        )\n",
    "    except TypeError:\n",
    "        cat.fit(Pool(X_tr, y_tr), eval_set=Pool(X_val, y_val), use_best_model=True)\n",
    "# making predictions\n",
    "    cat_pred = cat.predict(X_test)\n",
    "    m = eval_preds(y_test, cat_pred); m[\"Model\"] = \"CatBoost\"\n",
    "    metrics.append(m)\n",
    "    preds[\"CAT_Pred\"] = cat_pred\n",
    "    try:\n",
    "        fi = cat.get_feature_importance(Pool(X_tr, y_tr))\n",
    "        pd.Series(fi, index=X.columns).sort_values(ascending=False)\\\n",
    "          .to_csv(\"cat_importance.csv\", header=[\"importance\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------- Save & print summary ----------------\n",
    "metrics_df = pd.DataFrame(metrics).set_index(\"Model\").sort_values(\"RMSE\")\n",
    "metrics_df.to_csv(\"model_compare_metrics.csv\")\n",
    "preds.to_csv(\"model_compare_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Test Set Metrics ===\")\n",
    "print(metrics_df if not metrics_df.empty else \"No models ran (check installs).\")\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - model_compare_metrics.csv\")\n",
    "print(\" - model_compare_predictions.csv\")\n",
    "for p in [\"rf_importance.csv\", \"xgb_importance.csv\", \"lgbm_importance.csv\", \"cat_importance.csv\"]:\n",
    "    if Path(p).exists():\n",
    "        print(f\" - {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Print metrics table ===\n",
    "print(\"\\n=== Model Performance Metrics ===\")\n",
    "print(metrics_df.sort_values(\"RMSE\"))\n",
    "\n",
    "# Best model\n",
    "best_model = metrics_df.loc[metrics_df[\"RMSE\"].idxmin(), \"Model\"]\n",
    "print(f\"\\nBest model by RMSE: {best_model}\")\n",
    "\n",
    "# making predictions\n",
    "# Find prediction column\n",
    "pred_col = None\n",
    "for col in preds_frame.columns:\n",
    "    if col.lower().startswith(best_model.lower().split(\"(\")[0].lower()):\n",
    "        pred_col = col\n",
    "        break\n",
    "\n",
    "# making predictions\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(preds_frame[\"Actual\"], preds_frame[pred_col], alpha=0.6)\n",
    "plt.plot([preds_frame[\"Actual\"].min(), preds_frame[\"Actual\"].max()],\n",
    "         [preds_frame[\"Actual\"].min(), preds_frame[\"Actual\"].max()],\n",
    "# making predictions\n",
    "         'r--', label=\"Perfect prediction\")\n",
    "plt.xlabel(\"Actual EFFR\")\n",
    "plt.ylabel(f\"Predicted EFFR ({best_model})\")\n",
    "plt.title(f\"Actual vs Predicted EFFR — {best_model}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BEST MODEL\n",
    "# \n",
    "# \n",
    "# # Random Forest tuned for small dataset (no early stopping anywhere)\n",
    "# - Loads features_effr_top25_clean.csv (falls back to features_effr_top25.csv)\n",
    "# - TimeSeries CV + RandomizedSearch over depth/leaves/max_features with many trees\n",
    "# - 80/20 time-based split for final test\n",
    "# making predictions\n",
    "# - Saves metrics, predictions, best params, and importance; plots actual vs predicted\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "IN_PATHS = [\"features_effr_top25_clean.csv\", \"features_effr_top25.csv\",\n",
    "            \"features_effr_data_extended.csv\", \"features_effr_data.csv\"]\n",
    "TARGET = \"Effective Federal Funds Rate\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "for p in IN_PATHS:\n",
    "    if Path(p).exists():\n",
    "# loading the dataset\n",
    "        df = pd.read_csv(p)\n",
    "        print(f\"Loaded: {p}\")\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find any features CSV.\")\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target '{TARGET}' not in dataframe.\")\n",
    "\n",
    "# Keep numeric features only; drop 'date' if present\n",
    "exclude = [\"date\", TARGET]\n",
    "X = df.drop(columns=[c for c in exclude if c in df.columns], errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "mask = y.notna()\n",
    "X, y = X.loc[mask], y.loc[mask]\n",
    "\n",
    "# Safety: finite values only\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# ---------- Time-based split ----------\n",
    "n = len(X)\n",
    "split_idx = int(n * 0.80)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}, Features: {X.shape[1]}\")\n",
    "\n",
    "# ---------- CV setup ----------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# ---------- Search space ----------\n",
    "param_dist = {\n",
    "    \"n_estimators\": np.arange(800, 3001, 200),     # many trees for stability on small data\n",
    "    \"max_depth\": np.append(np.arange(4, 25, 2), [None]),\n",
    "    \"min_samples_leaf\": np.arange(1, 8),           # smaller leaves capture nonlinearity\n",
    "    \"min_samples_split\": np.arange(2, 10),\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.7, None],\n",
    "    \"bootstrap\": [True],                           # OOB-style sampling tends to help\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=120,                # bump if you want more thorough search\n",
    "    cv=tscv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_rf = search.best_estimator_\n",
    "print(\"\\nBest RF params:\")\n",
    "print(search.best_params_)\n",
    "\n",
    "# ---------- Evaluate on test ----------\n",
    "# making predictions\n",
    "pred = best_rf.predict(X_test)\n",
    "\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, pred)))\n",
    "mae  = float(mean_absolute_error(y_test, pred))\n",
    "r2   = float(r2_score(y_test, pred))\n",
    "\n",
    "print(\"\\n=== Test Set (RandomForest Tuned) ===\")\n",
    "print(f\"RMSE: {rmse:.4f}  MAE: {mae:.4f}  R2: {r2:.4f}\")\n",
    "\n",
    "# ---------- Save artifacts ----------\n",
    "metrics = {\"Model\": \"RandomForest_Tuned\",\n",
    "           \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "pd.DataFrame([metrics]).to_csv(\"rf_tuned_metrics.csv\", index=False)\n",
    "\n",
    "preds = pd.DataFrame({\n",
    "    \"Actual\": y_test.values,\n",
    "    \"RF_Tuned_Pred\": pred\n",
    "}, index=X_test.index).reset_index(drop=True)\n",
    "# making predictions\n",
    "preds.to_csv(\"rf_tuned_predictions.csv\", index=False)\n",
    "\n",
    "imp = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "imp.to_csv(\"rf_tuned_feature_importance.csv\", header=[\"importance\"])\n",
    "\n",
    "with open(\"rf_tuned_best_params.json\", \"w\") as f:\n",
    "    # ensure JSON-serializable types\n",
    "    bp = {k: (int(v) if isinstance(v, (np.integer,)) else float(v) if isinstance(v, (np.floating,))\n",
    "              else (None if v is None else v))\n",
    "          for k, v in search.best_params_.items()}\n",
    "    json.dump(bp, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - rf_tuned_metrics.csv\")\n",
    "# making predictions\n",
    "print(\" - rf_tuned_predictions.csv\")\n",
    "print(\" - rf_tuned_feature_importance.csv\")\n",
    "print(\" - rf_tuned_best_params.json\")\n",
    "\n",
    "# ---------- Plot Actual vs Predicted ----------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(preds[\"Actual\"], preds[\"RF_Tuned_Pred\"], alpha=0.65)\n",
    "a_min, a_max = preds[\"Actual\"].min(), preds[\"Actual\"].max()\n",
    "# making predictions\n",
    "plt.plot([a_min, a_max], [a_min, a_max], linestyle=\"--\", label=\"Perfect prediction\")\n",
    "plt.xlabel(\"Actual EFFR\")\n",
    "plt.ylabel(\"Predicted EFFR (RF Tuned)\")\n",
    "plt.title(\"Actual vs Predicted EFFR — RandomForest (Tuned)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d561f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load feature importances\n",
    "file_path = \"rf_tuned_feature_importance.csv\"\n",
    "# loading the dataset\n",
    "importances = pd.read_csv(file_path, index_col=0).squeeze()  # converts to Series\n",
    "\n",
    "# Plot top 25\n",
    "top_n = 25\n",
    "top_features = importances.sort_values(ascending=True).tail(top_n)  # ascending for horizontal bar\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 10))\n",
    "top_features.plot(kind='barh', color='skyblue')\n",
    "plt.title(f\"Top {top_n} Feature Importances (Random Forest Tuned)\", fontsize=14)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391b350",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The Random Forest model performed well at capturing general trends in EFFR, especially in mid-range values. However, it struggled with extreme changes due to limited examples in the data.\n",
    "\n",
    "Future improvements could include:\n",
    "- Incorporating additional macroeconomic indicators\n",
    "- Using more recent or real-time data\n",
    "- Applying time-series-specific models like ARIMA or LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
